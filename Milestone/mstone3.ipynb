{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":127742,"databundleVersionId":15295088,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:28:16.044000Z","iopub.execute_input":"2026-02-08T14:28:16.044355Z","iopub.status.idle":"2026-02-08T14:28:16.344614Z","shell.execute_reply.started":"2026-02-08T14:28:16.044328Z","shell.execute_reply":"2026-02-08T14:28:16.343704Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/comment-category-prediction-challenge/Sample.csv\n/kaggle/input/comment-category-prediction-challenge/train.csv\n/kaggle/input/comment-category-prediction-challenge/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Dimentionality Reduction","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n?PCA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:29:59.673474Z","iopub.execute_input":"2026-02-08T14:29:59.673851Z","iopub.status.idle":"2026-02-08T14:29:59.711216Z","shell.execute_reply.started":"2026-02-08T14:29:59.673820Z","shell.execute_reply":"2026-02-08T14:29:59.710422Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mwhiten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msvd_solver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0miterated_power\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_oversamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpower_iteration_normalizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nPrincipal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nWith sparse inputs, the ARPACK implementation of the truncated SVD can be\nused (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one\nmay consider :class:`TruncatedSVD` where the data are not centered.\n\nNotice that this class only supports sparse inputs for some solvers such as\n\"arpack\" and \"covariance_eigh\". See :class:`TruncatedSVD` for an\nalternative with sparse data.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'\n    \"auto\" :\n        The solver is selected by a default 'auto' policy is based on `X.shape` and\n        `n_components`: if the input data has fewer than 1000 features and\n        more than 10 times as many samples, then the \"covariance_eigh\"\n        solver is used. Otherwise, if the input data is larger than 500x500\n        and the number of components to extract is lower than 80% of the\n        smallest dimension of the data, then the more efficient\n        \"randomized\" method is selected. Otherwise the exact \"full\" SVD is\n        computed and optionally truncated afterwards.\n    \"full\" :\n        Run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    \"covariance_eigh\" :\n        Precompute the covariance matrix (on centered data), run a\n        classical eigenvalue decomposition on the covariance matrix\n        typically using LAPACK and select the components by postprocessing.\n        This solver is very efficient for n_samples >> n_features and small\n        n_features. It is, however, not tractable otherwise for large\n        n_features (large memory footprint required to materialize the\n        covariance matrix). Also note that compared to the \"full\" solver,\n        this solver effectively doubles the condition number and is\n        therefore less numerical stable (e.g. on input data with a large\n        range of singular values).\n    \"arpack\" :\n        Run SVD truncated to `n_components` calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        `0 < n_components < min(X.shape)`\n    \"randomized\" :\n        Run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\n    .. versionchanged:: 1.5\n        Added the 'covariance_eigh' solver.\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nn_oversamples : int, default=10\n    This parameter is only relevant when `svd_solver=\"randomized\"`.\n    It corresponds to the additional number of random vectors to sample the\n    range of `X` so as to ensure proper conditioning. See\n    :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n    .. versionadded:: 1.1\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Power iteration normalizer for randomized SVD solver.\n    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n    for more details.\n\n    .. versionadded:: 1.1\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. Equivalently, the right singular\n    vectors of the centered input data, parallel to its eigenvectors.\n    The components are sorted by decreasing ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    The variance estimation uses `n_samples - 1` degrees of freedom.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method from:\n`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\nIn NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\nImplements the probabilistic PCA model from:\n`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\n<http://www.miketipping.com/papers/met-mppca.pdf>`_\nvia the score and score_samples methods.\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n:doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.\n<10.1137/090771806>`\nand also\n:doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68.\n<10.1016/j.acha.2010.02.003>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m     \n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"X=np.array([[1,3,4,5,6],[45,6,6,7,7],[4,6,7,8,9],[9,0,67,8,9],[1,3,4,5,6],[4,6,7,8,1]])\ndim_reduction=PCA(n_components=2)\ndim_reduction.fit(X)\nX.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:55:58.463649Z","iopub.execute_input":"2026-02-08T14:55:58.464028Z","iopub.status.idle":"2026-02-08T14:55:58.474990Z","shell.execute_reply.started":"2026-02-08T14:55:58.463994Z","shell.execute_reply":"2026-02-08T14:55:58.474351Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(6, 5)"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"a=dim_reduction.components_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:57:35.323711Z","iopub.execute_input":"2026-02-08T14:57:35.324001Z","iopub.status.idle":"2026-02-08T14:57:35.328322Z","shell.execute_reply.started":"2026-02-08T14:57:35.323976Z","shell.execute_reply":"2026-02-08T14:57:35.327384Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## MODEL Retraning","metadata":{}},{"cell_type":"code","source":"Y=a\nY=pd.DataFrame(Y)\nY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:58:47.073505Z","iopub.execute_input":"2026-02-08T14:58:47.073802Z","iopub.status.idle":"2026-02-08T14:58:47.096067Z","shell.execute_reply.started":"2026-02-08T14:58:47.073777Z","shell.execute_reply":"2026-02-08T14:58:47.095343Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"          0         1         2         3         4\n0 -0.045384 -0.075621  0.994492  0.024933  0.050841\n1  0.997352  0.039547  0.046256  0.019030  0.034971","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.045384</td>\n      <td>-0.075621</td>\n      <td>0.994492</td>\n      <td>0.024933</td>\n      <td>0.050841</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.997352</td>\n      <td>0.039547</td>\n      <td>0.046256</td>\n      <td>0.019030</td>\n      <td>0.034971</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:04:03.282969Z","iopub.execute_input":"2026-02-08T15:04:03.283266Z","iopub.status.idle":"2026-02-08T15:04:03.287187Z","shell.execute_reply.started":"2026-02-08T15:04:03.283239Z","shell.execute_reply":"2026-02-08T15:04:03.286358Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"new=np.array([[1],[3]])\nX1=Y\ny1=pd.DataFrame(new)\ny1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:03:16.583366Z","iopub.execute_input":"2026-02-08T15:03:16.583715Z","iopub.status.idle":"2026-02-08T15:03:16.590824Z","shell.execute_reply.started":"2026-02-08T15:03:16.583689Z","shell.execute_reply":"2026-02-08T15:03:16.590111Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   0\n0  1\n1  3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X1,y1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:04:05.363185Z","iopub.execute_input":"2026-02-08T15:04:05.363521Z","iopub.status.idle":"2026-02-08T15:04:05.372715Z","shell.execute_reply.started":"2026-02-08T15:04:05.363495Z","shell.execute_reply":"2026-02-08T15:04:05.371859Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import sklearn\ndir(sklearn.naive_bayes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:06:05.858539Z","iopub.execute_input":"2026-02-08T15:06:05.858813Z","iopub.status.idle":"2026-02-08T15:06:05.873344Z","shell.execute_reply.started":"2026-02-08T15:06:05.858792Z","shell.execute_reply":"2026-02-08T15:06:05.872417Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"['ABCMeta',\n 'BaseEstimator',\n 'BernoulliNB',\n 'CategoricalNB',\n 'ClassifierMixin',\n 'ComplementNB',\n 'GaussianNB',\n 'Integral',\n 'Interval',\n 'LabelBinarizer',\n 'MultinomialNB',\n 'Real',\n '_BaseDiscreteNB',\n '_BaseNB',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_check_n_features',\n '_check_partial_fit_first_call',\n '_check_sample_weight',\n '_fit_context',\n 'abstractmethod',\n 'binarize',\n 'check_is_fitted',\n 'check_non_negative',\n 'label_binarize',\n 'logsumexp',\n 'np',\n 'safe_sparse_dot',\n 'validate_data',\n 'warnings']"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:06:20.114021Z","iopub.execute_input":"2026-02-08T15:06:20.114300Z","iopub.status.idle":"2026-02-08T15:06:20.118049Z","shell.execute_reply.started":"2026-02-08T15:06:20.114277Z","shell.execute_reply":"2026-02-08T15:06:20.117240Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}